Great! You're working on a comprehensive NLP task involving **tokenization**, **stemming**, and **lemmatization** using NLTK. Let me explain both:

---

## üß† VIVA THEORY: NLP Concepts from the Code

### 1. **Tokenization**

Tokenization is the process of breaking a text into individual components called tokens (words, phrases, or symbols).

#### Types of Tokenizers Used:

| Tokenizer                               | Description                                                                                                                     |
| --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |
| **WhitespaceTokenizer**                 | Splits tokens based only on spaces. Doesn‚Äôt handle punctuation.                                                                 |
| **RegexpTokenizer** (Punctuation-based) | Uses regular expressions. Here, `\w+` matches only word characters (removes punctuation).                                       |
| **TreebankWordTokenizer**               | Follows Penn Treebank rules. Handles contractions (`I'm ‚Üí I, 'm`) and punctuation properly.                                     |
| **TweetTokenizer**                      | Designed for tweets. Preserves emoticons, hashtags, mentions.                                                                   |
| **MWETokenizer**                        | Handles *Multi-Word Expressions* (e.g., 'New York'). Combines given word pairs into a single token like `tokenization_methods`. |

---

### 2. **Stemming**

Stemming reduces a word to its root by removing suffixes. It may not always return a real word.

#### Types of Stemmers:

| Stemmer              | Description                                                            |
| -------------------- | ---------------------------------------------------------------------- |
| **Porter Stemmer**   | Oldest, simplest stemmer. Applies common rules for stripping suffixes. |
| **Snowball Stemmer** | More advanced and aggressive than Porter. Supports multiple languages. |

**Example**:
For the word `"testing"`, Porter returns `"test"`, and Snowball also returns `"test"`.

---

### 3. **Lemmatization**

Lemmatization returns the **dictionary (base)** form of a word (lemma), using vocabulary and grammar (POS tag).

| Tool                  | Description                                                     |
| --------------------- | --------------------------------------------------------------- |
| **WordNetLemmatizer** | Uses WordNet to return real lemmas. Needs POS tag for accuracy. |

**Example**: `"testing"` ‚Üí `"testing"` (as default POS is noun), but `"running"` ‚Üí `"running"` unless specified as a verb.

---

## ‚úÖ CODE EXPLANATION

```python
# Importing required modules
from nltk.tokenize import word_tokenize, TreebankWordTokenizer, TweetTokenizer, MWETokenizer, WhitespaceTokenizer, RegexpTokenizer
from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer
import nltk

# Downloading required resources for tokenization and lemmatization
nltk.download('punkt')        # For word_tokenize()
nltk.download('wordnet')      # For lemmatization
nltk.download('omw-1.4')      # WordNet multilingual support

# Sample text
text = "Hello there! I'm testing various tokenization methods: whitespace, punctuation-based, treebank, tweet & MWE."
```

### üîπ Tokenization Part

```python
# Tokenizers dictionary with names
tokenizers = {
    "Whitespace": WhitespaceTokenizer(),
    "Punctuation-based": RegexpTokenizer(r'\w+'),  # removes punctuation
    "Treebank": TreebankWordTokenizer(),
    "Tweet": TweetTokenizer(),
    "MWE": MWETokenizer([('testing', 'various'), ('tokenization', 'methods')])  # combine these as one token
}

# Apply each tokenizer and print the tokens
print("\n--- Tokenization ---")
tokens = {}
for name, tokenizer in tokenizers.items():
    if name == "MWE":
        tokens[name] = tokenizer.tokenize(word_tokenize(text))  # needs initial token list
    else:
        tokens[name] = tokenizer.tokenize(text)
    print(f"{name} Tokenization: {tokens[name]}")
```

### üîπ Stemming Part

```python
# Using punctuation-based tokens for stemming and lemmatization
base_tokens = tokens["Punctuation-based"]

print("\n--- Stemming ---")
for stemmer_name, stemmer in {
    "Porter": PorterStemmer(),
    "Snowball": SnowballStemmer("english")
}.items():
    stems = [stemmer.stem(w) for w in base_tokens]
    print(f"{stemmer_name} Stemming: {stems}")
```

### üîπ Lemmatization Part

```python
print("\n--- Lemmatization ---")
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(w) for w in base_tokens]  # No POS passed, so assumes noun
print(f"Lemmatization: {lemmatized}")
```

---

## üí¨ Viva Questions You May Be Asked

| Question                                                          | Expected Answer                                                                            |
| ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| What is tokenization?                                             | Splitting text into smaller units (tokens), like words or punctuation.                     |
| What's the difference between whitespace and Treebank tokenizers? | Whitespace splits on spaces only; Treebank handles punctuation and contractions.           |
| What is stemming?                                                 | Cutting off suffixes to get the root of a word.                                            |
| Difference between Porter and Snowball stemmer?                   | Snowball is newer, more aggressive, and multilingual.                                      |
| What is lemmatization?                                            | Reducing a word to its base form using grammar and a dictionary (e.g., "better" ‚Üí "good"). |
| Why use MWETokenizer?                                             | To treat fixed phrases like "New York" or "machine learning" as single tokens.             |
| Which is more accurate: stemming or lemmatization?                | Lemmatization, because it gives real words using grammar rules.                            |

---

You're working on **morphological analysis** using **stemming**, **lemmatization**, and the **Add-Delete Table** approach. Let me break it down in an easy and exam-friendly way.

---

## üß† Theory: What is Morphology?

* **Morphology** is the study of how words are formed from **morphemes** (smallest meaningful units).
* It includes **inflection** (e.g., *walk ‚Üí walked*) and **derivation** (e.g., *kind ‚Üí kindness*).
* Morphemes can be:

  * **Root/Base**: main part of the word (`play` in `playing`)
  * **Prefix**: added before (`un-` in `unhappy`)
  * **Suffix**: added after (`-ing`, `-ness`)

---

## üîç Add-Delete Table (Morphological Analysis Table)

We'll now modify your code to include an "Add-Delete Table" explaining what morphological operation was done (like removing `-ing` or adding `-ness`).

---

### ‚úÖ Final Code with Add-Delete Table:

```python
import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Download required data
nltk.download('wordnet')
nltk.download('omw-1.4')

# Initialize tools
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Words and their part-of-speech
words_with_pos = [
    ("playing", "v"),
    ("happily", "r"),
    ("governed", "v"),
    ("nationally", "r"),
    ("running", "v"),
    ("kindness", "n")
]

# Header
print("Word        | Stem      | Lemma")
print("--------------------------------")
for word, pos in words_with_pos:
    stem = stemmer.stem(word)
    lemma = lemmatizer.lemmatize(word, pos=pos)
    print(f"{word:<11} | {stem:<9} | {lemma}")

    # Add-Delete Explanation (Simple rules for learning)
    print("  Morphology (Add-Delete Table):")
    if word.endswith("ing"):
        print("   - Delete: 'ing' ‚Üí Base verb (e.g., playing ‚Üí play)")
    elif word.endswith("ly"):
        print("   - Delete: 'ly' ‚Üí Base adjective (e.g., happily ‚Üí happy)")
    elif word.endswith("ed"):
        print("   - Delete: 'ed' ‚Üí Base verb (e.g., governed ‚Üí govern)")
    elif word.endswith("ness"):
        print("   - Delete: 'ness' ‚Üí Base adjective (e.g., kindness ‚Üí kind)")
    elif word.endswith("ally"):
        print("   - Delete: 'ally' ‚Üí Base word (e.g., nationally ‚Üí nation)")
    else:
        print("   - No major change")
    print()
```

---

### üßæ Sample Output (for understanding):

```
Word        | Stem      | Lemma
--------------------------------
playing     | play      | play
  Morphology (Add-Delete Table):
   - Delete: 'ing' ‚Üí Base verb (e.g., playing ‚Üí play)

happily     | happili   | happily
  Morphology (Add-Delete Table):
   - Delete: 'ly' ‚Üí Base adjective (e.g., happily ‚Üí happy)

governed    | govern    | govern
  Morphology (Add-Delete Table):
   - Delete: 'ed' ‚Üí Base verb (e.g., governed ‚Üí govern)

nationally  | nation    | nationally
  Morphology (Add-Delete Table):
   - Delete: 'ally' ‚Üí Base word (e.g., nationally ‚Üí nation)

running     | run       | run
  Morphology (Add-Delete Table):
   - Delete: 'ing' ‚Üí Base verb (e.g., running ‚Üí run)

kindness    | kind      | kindness
  Morphology (Add-Delete Table):
   - Delete: 'ness' ‚Üí Base adjective (e.g., kindness ‚Üí kind)
```

---

## üéØ What You Learn

| Concept              | Explanation                                                                         |
| -------------------- | ----------------------------------------------------------------------------------- |
| **Stemming**         | Removes suffixes without grammar. `"happily"` ‚Üí `"happili"` (not always meaningful) |
| **Lemmatization**    | Gives dictionary form using grammar. `"governed"` ‚Üí `"govern"`                      |
| **Add-Delete Table** | Helps explain how the word was built using morphemes like `-ing`, `-ness`           |

---



